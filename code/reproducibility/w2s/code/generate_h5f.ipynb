{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "\n",
    "class HDF5Store(object):\n",
    "    \"\"\"\n",
    "    Simple class to append value to a hdf5 file on disc (usefull for building keras datasets)\n",
    "    \n",
    "    Params:\n",
    "        datapath: filepath of h5 file\n",
    "        dataset: dataset name within the file\n",
    "        shape: dataset shape (not counting main/batch axis)\n",
    "        dtype: numpy dtype\n",
    "    \n",
    "    Usage:\n",
    "        hdf5_store = HDF5Store('/tmp/hdf5_store.h5','X', shape=(20,20,3))\n",
    "        x = np.random.random(hdf5_store.shape)\n",
    "        hdf5_store.append(x)\n",
    "        hdf5_store.append(x)\n",
    "        \n",
    "    \"\"\"\n",
    "    def __init__(self, datapath, dataset, shape, dtype=np.float32, compression=\"gzip\", chunk_len=1):\n",
    "        self.datapath = datapath\n",
    "        self.dataset = dataset\n",
    "        self.shape = shape\n",
    "        self.i = 0\n",
    "        \n",
    "        with h5py.File(self.datapath, mode='w') as h5f:\n",
    "            self.dset = h5f.create_dataset(\n",
    "                dataset,\n",
    "                shape=(0, ) + shape,\n",
    "                maxshape=(None, ) + shape,\n",
    "                dtype=dtype,\n",
    "                compression=compression,\n",
    "                chunks=(chunk_len, ) + shape)\n",
    "    \n",
    "    def append(self, values):\n",
    "        with h5py.File(self.datapath, mode='a') as h5f:\n",
    "            dset = h5f[self.dataset]\n",
    "            dset.resize((self.i + 1, ) + self.shape)\n",
    "            dset[self.i] = [values]\n",
    "            self.i += 1\n",
    "            h5f.flush()\n",
    "\n",
    "\n",
    "\n",
    "def generate_h5f(data_path, folder_name, patch_size, stride, number_of_training_images=240):\n",
    "    '''\n",
    "    This function generates all h5 files and stores them in 'net_data/' with name: f'{folder_name}_{patch_size}_{stride}'\n",
    "    \n",
    "        data_path: where the image folders are stored, each folder is avg1, avgX etc, +sim\n",
    "        folder_name: picks between avg1, sim, etc\n",
    "        number_of_training_images: how many images to use for the training set\n",
    "        \n",
    "        Ex: data_path = '../data/all' + folder_name = 'avg1'\n",
    "        Ex: data_path = '../results/BM3D' + folder_name = 'avg1'\n",
    "    '''\n",
    "    \n",
    "    print(f'Creating training h5f, for {folder_name}...')\n",
    "        \n",
    "    #OLD normalization: files = glob.glob(os.path.join(data_path, folder_name, '*.png'))\n",
    "    files = glob.glob(os.path.join(data_path, folder_name, '*.npy'))\n",
    "    files.sort()\n",
    "    if number_of_training_images > len(files):\n",
    "        raise NotImplementedError(f'Maximum available images: {len(files)}.')\n",
    "\n",
    "\n",
    "    train_file_name = f'{folder_name}_{patch_size}_{stride}'\n",
    "    patch_shape = (1, patch_size, patch_size)\n",
    "    hdf5 = HDF5Store(datapath=os.path.join('../net_data/',train_file_name) + '.h5', dataset='data', shape=patch_shape)\n",
    "\n",
    "    num_patches = 0\n",
    "    for idx in range(number_of_training_images):\n",
    "        #OLD normalization: img = cv2.imread(files[idx], cv2.IMREAD_GRAYSCALE)\n",
    "        img = np.load(files[idx])\n",
    "        (h, w) = np.shape(img)\n",
    "\n",
    "        idx_x = 0\n",
    "        while (idx_x + patch_size < h):\n",
    "            idx_y = 0\n",
    "            while (idx_y + patch_size < w):\n",
    "                patch = img[idx_x:idx_x+patch_size, idx_y:idx_y+patch_size]\n",
    "                hdf5.append(patch/255.)\n",
    "                num_patches = num_patches + 1\n",
    "\n",
    "                idx_y = idx_y + stride\n",
    "\n",
    "            idx_x = idx_x + stride\n",
    "\n",
    "    print('%d patches generated, and saved.'%(num_patches))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the generation of all h5f with specified settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training h5f, for avg400...\n",
      "47040 patches generated, and saved.\n",
      "Creating training h5f, for sim...\n",
      "47040 patches generated, and saved.\n",
      "Creating training h5f, for avg1...\n",
      "47040 patches generated, and saved.\n",
      "Creating training h5f, for avg2...\n",
      "47040 patches generated, and saved.\n",
      "Creating training h5f, for avg4...\n",
      "47040 patches generated, and saved.\n",
      "Creating training h5f, for avg8...\n",
      "47040 patches generated, and saved.\n",
      "Creating training h5f, for avg16...\n",
      "47040 patches generated, and saved.\n"
     ]
    }
   ],
   "source": [
    "data_path = '../data/train/'\n",
    "number_of_training_images = 240\n",
    "\n",
    "\n",
    "for folder_name in ['avg400','sim','avg1','avg2','avg4','avg8','avg16']:\n",
    "\n",
    "    patch_size = 64\n",
    "    stride = 32\n",
    "    if folder_name == 'sim':\n",
    "        patch_size = patch_size * 2\n",
    "        stride = stride * 2\n",
    "    \n",
    "    generate_h5f(data_path, folder_name, patch_size, stride, number_of_training_images)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
